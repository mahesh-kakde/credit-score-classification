{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SAXr9uW7ixuZ"
      },
      "source": [
        "# DSN2099 - Project Exhibition II\n",
        "## Mahesh Kakde"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nAAF5ZrOi2J2"
      },
      "source": [
        "The dataset I am using is hosted on Kaggle - https://www.kaggle.com/datasets/maheshkakde165/credit-score-data\n",
        "\n",
        "### Credit Score Class Classification\n",
        "\n",
        "### Goal\n",
        "Given a personâ€™s credit-related information, build a machine learning model that can classify the credit score. There are three classes - Standard, Good and Poor, that we have to predict.\n",
        "\n",
        "I will encode the classes as below:\n",
        "1 - Poor,\n",
        "2 - Standard,\n",
        "3 - Good"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oxGEs9qLivt7"
      },
      "outputs": [],
      "source": [
        "# Importing necessary libraries and modules\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Ignore Warnings\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oJDlaP9cjr00"
      },
      "outputs": [],
      "source": [
        "# load the dataset\n",
        "df = pd.read_csv(\"credit_score_data.csv\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "df.shape"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "There are 2500 records over 22 columns."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yh8VLt2ojs9M",
        "outputId": "690f861e-4cee-44f1-d8a2-76a94dd0fa46"
      },
      "outputs": [],
      "source": [
        "df.info()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p_ieO-IMngE1"
      },
      "source": [
        "## 1. Statistical descriptions and Visualizations"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mqOzUy4coi3a"
      },
      "source": [
        "There are four categorical columns including the target:\n",
        "* Credit_Score - Target\n",
        "* Payment_of_Min_Amount - Not Ordinal\n",
        "* Credit_Mix - Ordinal\n",
        "* Payment_Behaviour - Ordinal\n",
        "\n",
        "We will use on-hot encoding to turn them to numerical."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HrFOKRhnmLN9"
      },
      "outputs": [],
      "source": [
        "num_features = ['Delay_from_due_date', 'Num_of_Delayed_Payment', 'Num_Credit_Inquiries',\n",
        "       'Credit_Utilization_Ratio', 'Credit_History_Age',\n",
        "        'Amount_invested_monthly', 'Monthly_Balance',\n",
        "       'Age','Annual_Income', 'Num_Bank_Accounts', 'Num_Credit_Card',\n",
        "       'Interest_Rate', 'Num_of_Loan', 'Monthly_Inhand_Salary',\n",
        "       'Changed_Credit_Limit', 'Outstanding_Debt', 'Total_EMI_per_month']\n",
        "cat_features = ['Payment_of_Min_Amount', 'Credit_Score', 'Credit_Mix', 'Payment_Behaviour']"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aRQA-cJboD8q"
      },
      "source": [
        "Encode the target classes as:\n",
        "1 - Poor,\n",
        "2 - Standard,\n",
        "3 - Good"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EyLHv0MonkDF"
      },
      "outputs": [],
      "source": [
        "mapping = {'Poor': 1, 'Standard': 2, 'Good': 3}\n",
        "df['Credit_Score'] = df['Credit_Score'].replace(mapping)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "84iTQuIyp3TC"
      },
      "source": [
        "Transforming the Ordinal Categorical Features to Numerical"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gJqA8zXSo15k"
      },
      "outputs": [],
      "source": [
        "# Ordinal Encoding\n",
        "from sklearn.preprocessing import OrdinalEncoder\n",
        "\n",
        "# Payment Behavior\n",
        "categories_PB = [\"Low_spent_Small_value_payments\", \"Low_spent_Medium_value_payments\", \"Low_spent_Large_value_payments\",\n",
        "                    \"High_spent_Small_value_payments\", \"High_spent_Medium_value_payments\", \"High_spent_Large_value_payments\"]\n",
        "\n",
        "encoder1 = OrdinalEncoder(categories=[categories_PB])\n",
        "df[\"Payment_Behaviour\"] = encoder1.fit_transform(df[[\"Payment_Behaviour\"]])\n",
        "\n",
        "# Credit Mix\n",
        "categories_CM = [\"Bad\", \"Standard\", \"Good\"]\n",
        "\n",
        "encoder1 = OrdinalEncoder(categories=[categories_CM])\n",
        "df[\"Credit_Mix\"] = encoder1.fit_transform(df[[\"Credit_Mix\"]])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4gY_3HDFrRQT"
      },
      "source": [
        "Now, transforming the non-ordinal feature 'Payment_of_Min_Amount' to Numerical using One-Hot Encoding."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YIeyTwsIq2gy"
      },
      "outputs": [],
      "source": [
        "from sklearn.preprocessing import OneHotEncoder\n",
        "\n",
        "encoder = OneHotEncoder()\n",
        "\n",
        "encoded_data = encoder.fit_transform(df[[\"Payment_of_Min_Amount\"]])\n",
        "encoded_df = pd.DataFrame(encoded_data.toarray(), columns=encoder.get_feature_names_out(['Payment_of_Min_Amount']))\n",
        "df = pd.concat([df.drop([\"Payment_of_Min_Amount\"], axis=1), encoded_df], axis=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 350
        },
        "id": "jZ9suPwSrpcs",
        "outputId": "0cb09814-5343-43fb-e765-45fdffa6b6df"
      },
      "outputs": [],
      "source": [
        "df.describe()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QC4fZG3GsCxr",
        "outputId": "01a77dc4-3d02-4b16-dd15-7eed705aebc1"
      },
      "outputs": [],
      "source": [
        "df.info()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "All the features are now numerical and ready to be fed to machine learning models."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 256
        },
        "id": "r62OvVozsHAe",
        "outputId": "9580ba39-3e52-4b25-dc3a-05df5f77c40d"
      },
      "outputs": [],
      "source": [
        "df.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V5xalnKjsKLY"
      },
      "source": [
        "Now, the dataset has only numerical features. Let's visualize the data using Histograms"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "dFRNNMuFsIUT",
        "outputId": "98785b55-0e6e-4755-d601-7097df4e611f"
      },
      "outputs": [],
      "source": [
        "# Visualization - Histogram\n",
        "df.hist(figsize=(20,15))\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qBVNDqe1sgdi"
      },
      "source": [
        "Let's use Box Plots to get a picture of outliers."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "btWqbbSUsTrX",
        "outputId": "96702ad9-867a-4199-d460-b82c2574e298"
      },
      "outputs": [],
      "source": [
        "# Visualization - Boxplots\n",
        "\n",
        "for column in df.columns:\n",
        "    plt.figure(figsize=(6, 4))\n",
        "    sns.boxplot(x=df[column])\n",
        "    plt.title('Boxplot of {}'.format(column))\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cC8ktPjrtYiO"
      },
      "source": [
        "#### Observations:\n",
        "\n",
        "The following patterns were observed in the histogram visualization of the dataset:\n",
        "-- Well Balanced (Close to Normal)\n",
        "* Num_of_Delayed_Payment\n",
        "* Credit_Utilization_Ratio\n",
        "* Credit_History_Age\n",
        "* Age\n",
        "* Num_Credit_Card\n",
        "\n",
        "-- Left Skewed Distributions\n",
        "* Delay_from_due_date\n",
        "* Num_Credit_Inquiries\n",
        "* Amount_invested_monthly\n",
        "* Monthly_Balance\n",
        "* Annual_Income\n",
        "* Interest_Rate\n",
        "* Num_of_Loan\n",
        "* Monthly_Inhand_Salary\n",
        "* Changed_Credit_Limit\n",
        "* Outstanding_Debt\n",
        "* Total_EMI_per_month\n",
        "\n",
        "-- Right Skewed Distributions\n",
        "* Num_Bank_Accounts\n",
        "\n",
        "-- Based on the Boxplots, many Outliers are detected in the foloowing features:\n",
        "* Total_EMI_per_month\n",
        "* Outstanding_Debt\n",
        "* Changed_Credit_Limit\n",
        "* Monthly_Inhand_Salary\n",
        "* Annual_Income\n",
        "* Monthly_Balance\n",
        "* Amount_invested_monthly\n",
        "* Num_Credit_Inquiries\n",
        "* Delay_from_due_date\n",
        "\n",
        "We should not transform the data and remove outliers as they are important for the predictions of the credit score.\n",
        "But their skewed distribution and outliers will bring the model performance down. To mitigate this problem we will perform feature scaling using Standard Scaler in the sklearn library.\n",
        "\n",
        "#### Special Treatments\n",
        "We have used One-Hot encoding and Ordinal Encoding on the appriate features and later we will be using the standard scaler from sklearn library once we split the dataset into training, validation, and testing sets."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NFZ01B7fv79r"
      },
      "source": [
        "## 2. Correlation and Scatter Plots"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CS4NOeVjtYXV"
      },
      "outputs": [],
      "source": [
        "# Calculating PCC\n",
        "corr_matrix = df.corr()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "w5y2E1GWsmjL",
        "outputId": "4095cbbd-a30b-494b-a8f7-2769c2f404bf"
      },
      "outputs": [],
      "source": [
        "corr_matrix[\"Credit_Score\"].sort_values(ascending=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 888
        },
        "id": "5TfWApbHwL7l",
        "outputId": "fcf81ed3-af90-49e1-ba0f-4a8e4095cb27"
      },
      "outputs": [],
      "source": [
        "# Heatmap for visualization\n",
        "plt.figure(figsize=(15, 8))\n",
        "sns.heatmap(corr_matrix, annot=True, cmap='coolwarm', fmt=\".2f\")\n",
        "plt.title('Correlation Heatmap')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tnPBFiQEzdTo"
      },
      "source": [
        "For Scatter plots, we don't need to plot all the pairs with each other. There will be too many if we plot scatter plots of each feature with every other feature. We will plot for features that have correlation of more than 60% with each other and all feature with the target."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BpvPDW3i3r8I",
        "outputId": "1c890996-83f3-405a-ed6d-f4eb6c7c56a9"
      },
      "outputs": [],
      "source": [
        "# Get all the features other than the target that have more than 60% PCC\n",
        "features = df.columns\n",
        "non_target_features = [feature for feature in features if feature != 'Credit_Score']\n",
        "\n",
        "pairs_above_threshold_non_target = []\n",
        "for i in range(len(non_target_features)):\n",
        "    for j in range(i+1, len(non_target_features)):  # i+1 to only consider upper triangle\n",
        "        if abs(corr_matrix.loc[non_target_features[i], non_target_features[j]]) > 0.6:\n",
        "            pairs_above_threshold_non_target.append((non_target_features[i], non_target_features[j]))\n",
        "\n",
        "pairs_above_threshold_non_target"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "Q7VixWbhwrUa",
        "outputId": "e6e34200-ac3f-4407-de07-9238c3746a63"
      },
      "outputs": [],
      "source": [
        "for (feature_x, feature_y) in pairs_above_threshold_non_target:\n",
        "    sns.scatterplot(x=df[feature_x], y=df[feature_y])\n",
        "    plt.title(f'Scatter Plot of {feature_x} vs {feature_y}')\n",
        "    plt.xlabel(feature_x)\n",
        "    plt.ylabel(feature_y)\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XMfHZFOm4n6L"
      },
      "source": [
        "Now, we will plot scatter plots of all features with the target - Credit_Score."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "8Q8EN-4U4jPl",
        "outputId": "dbc263d7-5d47-4bb1-be1e-c9545042cd5d"
      },
      "outputs": [],
      "source": [
        "non_target_features = [feature for feature in df.columns if feature != 'Credit_Score']\n",
        "\n",
        "for feature in non_target_features:\n",
        "    sns.scatterplot(x=df[feature], y=df['Credit_Score'])\n",
        "    plt.title(f'Scatter Plot of {feature} vs Credit_Score')\n",
        "    plt.xlabel(feature)\n",
        "    plt.ylabel('Credit_Score')\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4ERV2ONNwrgj"
      },
      "source": [
        "Observations:\n",
        "* No feature seems to have very high correlation (more than 0.8) with the target.\n",
        "* Among the negatively correlated features 'Num_Credit_Inquiries', 'Interest_Rate', 'Outstanding_Debt', 'Delay_from_due_date', 'Num_of_Loan', 'Num_Credit_Card' are strongly correlated with the target.\n",
        "* Among the positvely correlated features 'Credit_History_Age' has the highest correlation with the target."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nfPzu3wFyPA0"
      },
      "source": [
        "## 3. Dataset Split into Training, Validation, and Test"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tMOhfyduyvCL"
      },
      "source": [
        "We will split the dataset into 20% Test, 20% Validation, and 60% Training by employing the Stratified Sampling in sklearn library.\n",
        "\n",
        "We will first split the dataset into 40% training and 40% Test+Validate.\n",
        "Then, we will split the 40% Test+Validate into 20% Test and 20% Validate."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yNJTuagJyucL",
        "outputId": "3ceb1163-8ff0-468c-bcf6-043306880a9e"
      },
      "outputs": [],
      "source": [
        "df[['Credit_Score']].value_counts()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bC-sASoPw0y3",
        "outputId": "dfda8c8d-a876-42a7-d9e0-2813cef3a7bc"
      },
      "outputs": [],
      "source": [
        "from sklearn.model_selection import StratifiedShuffleSplit\n",
        "\n",
        "\n",
        "strat_split = StratifiedShuffleSplit(n_splits=1, test_size=0.4, random_state=42)\n",
        "\n",
        "# Generate the indices and split the data into training and test + validation sets\n",
        "for train_index, test_valid_index in strat_split.split(df, df['Credit_Score']):\n",
        "    strat_train_set = df.iloc[train_index]\n",
        "    strat_test_valid_set = df.iloc[test_valid_index]\n",
        "\n",
        "# Instantiate another StratifiedShuffleSplit object to split the test and validation sets\n",
        "strat_split_test_valid = StratifiedShuffleSplit(n_splits=1, test_size=0.5, random_state=42)\n",
        "\n",
        "# Generate the indices and split the test + validation sets into test and validation sets\n",
        "for test_index, valid_index in strat_split_test_valid.split(strat_test_valid_set, strat_test_valid_set['Credit_Score']):\n",
        "    strat_test_set = strat_test_valid_set.iloc[test_index]\n",
        "    strat_valid_set = strat_test_valid_set.iloc[valid_index]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dGsReeUY8P9m"
      },
      "source": [
        "#### Verification of the split\n",
        "Let's start with verifying the shapes of the sets."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "strat_train_set.shape, strat_valid_set.shape, strat_test_set.shape"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The shapes of the training, validation and testing sets are correct.\n",
        "\n",
        "Let's visualize using stacked bar graphs."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# List of all the features\n",
        "features = [\n",
        "    'Delay_from_due_date',\n",
        "    'Num_of_Delayed_Payment',\n",
        "    'Num_Credit_Inquiries',\n",
        "    'Credit_Utilization_Ratio',\n",
        "    'Credit_History_Age',\n",
        "    'Amount_invested_monthly',\n",
        "    'Monthly_Balance',\n",
        "    'Credit_Score',\n",
        "    'Credit_Mix',\n",
        "    'Payment_Behaviour',\n",
        "    'Age',\n",
        "    'Annual_Income',\n",
        "    'Num_Bank_Accounts',\n",
        "    'Num_Credit_Card',\n",
        "    'Interest_Rate',\n",
        "    'Num_of_Loan',\n",
        "    'Monthly_Inhand_Salary',\n",
        "    'Changed_Credit_Limit',\n",
        "    'Outstanding_Debt',\n",
        "    'Total_EMI_per_month',\n",
        "    'Payment_of_Min_Amount_NM',\n",
        "    'Payment_of_Min_Amount_No',\n",
        "    'Payment_of_Min_Amount_Yes'\n",
        "]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Function to plot the stacked bar chart of the training, validation, and testing sets for visual inspection\n",
        "def plot_category_proportions(df, train_set, valid_set, test_set, features):\n",
        "    plt.figure(figsize=(14, 10))\n",
        "    n_features = len(features)\n",
        "    \n",
        "    for feature in features:\n",
        "        # Calculate the value counts for each dataset\n",
        "        full_counts = df[feature].value_counts(normalize=True).sort_index()\n",
        "        train_counts = train_set[feature].value_counts(normalize=True).sort_index()\n",
        "        valid_counts = valid_set[feature].value_counts(normalize=True).sort_index()\n",
        "        test_counts = test_set[feature].value_counts(normalize=True).sort_index()\n",
        "        \n",
        "        # Correctly union all indices\n",
        "        all_indices = full_counts.index.union(train_counts.index).union(valid_counts.index).union(test_counts.index)\n",
        "        \n",
        "        train_counts = train_counts.reindex(all_indices, fill_value=0)\n",
        "        valid_counts = valid_counts.reindex(all_indices, fill_value=0)\n",
        "        test_counts = test_counts.reindex(all_indices, fill_value=0)\n",
        "        \n",
        "        proportions = pd.DataFrame({\n",
        "            'Training': train_counts,\n",
        "            'Validation': valid_counts,\n",
        "            'Testing': test_counts\n",
        "        })\n",
        "        \n",
        "        # Plot\n",
        "        proportions.plot(kind='bar', stacked=True, figsize=(10, 6))\n",
        "        plt.title(f'Proportional Distribution of {feature} Across Sets')\n",
        "        plt.ylabel('Proportion of Total')\n",
        "        plt.xlabel(feature)\n",
        "        plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "plot_category_proportions(df, strat_train_set, strat_valid_set, strat_test_set, features)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The plots above show that the data is split correctly."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Rm1ztSBx82eK"
      },
      "source": [
        "Now, we will get the X and y dataframes for each of the training, testing and validation sets."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HEOst4Lr6-Ui"
      },
      "outputs": [],
      "source": [
        "# Training\n",
        "X_train = strat_train_set.drop('Credit_Score', axis=1)\n",
        "y_train = strat_train_set[\"Credit_Score\"]\n",
        "\n",
        "# Validation\n",
        "X_val = strat_valid_set.drop('Credit_Score', axis=1)\n",
        "y_val = strat_valid_set[\"Credit_Score\"]\n",
        "\n",
        "# Testing\n",
        "X_test = strat_test_set.drop('Credit_Score', axis=1)\n",
        "y_test = strat_test_set[\"Credit_Score\"]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7B5qSK5p91gH"
      },
      "source": [
        "Now, we can proceed with the model training."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fNFqm6PU93KR"
      },
      "source": [
        "## 4. Training Models"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "u4VApmsf-Ref"
      },
      "outputs": [],
      "source": [
        "# Feature Scaling using Standard Scaler\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "scaler = StandardScaler()\n",
        "\n",
        "X_train = pd.DataFrame(scaler.fit_transform(X_train))\n",
        "X_val = pd.DataFrame(scaler.transform(X_val))\n",
        "X_test = pd.DataFrame(scaler.transform(X_test))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FT4WV0dM-cf-"
      },
      "outputs": [],
      "source": [
        "# Function Calculate and print scores\n",
        "def compute_print_metrics(y_original, y_pred):\n",
        "    # Computing the metrics\n",
        "    acc = accuracy_score(y_original, y_pred)\n",
        "    pre = precision_score(y_original, y_pred, average='macro')\n",
        "    rec = recall_score(y_original, y_pred, average=\"macro\")\n",
        "    f1 = f1_score(y_original, y_pred, average=\"macro\")\n",
        "\n",
        "    # Print the metrics\n",
        "    print(\"Accuracy: \", acc)\n",
        "    print(\"Precision: \", pre)\n",
        "    print(\"Recall: \", rec)\n",
        "    print(\"F1 Score: \", f1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xqXc3P4Q94vd"
      },
      "source": [
        "### 4A Multinomial Logistic Regression"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Let's train a base model first"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cb-uA7IX9zdh",
        "outputId": "0772913e-cfd8-482b-9587-4b07f4a3bf56"
      },
      "outputs": [],
      "source": [
        "# Multinomial Logistic Regression (softmax regression)\n",
        "\n",
        "# Import statements\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
        "\n",
        "# Example for training a logistic regression model\n",
        "log_reg_initial = LogisticRegression(multi_class='multinomial',\n",
        "                             solver='newton-cg', C=100,\n",
        "                             max_iter=1)\n",
        "\n",
        "# Fit on training data\n",
        "log_reg_initial.fit(X_train, y_train)\n",
        "\n",
        "# Predict on validation data\n",
        "y_train_pred_log = log_reg_initial.predict(X_train)\n",
        "y_val_pred_log = log_reg_initial.predict(X_val)\n",
        "y_test_pred_log = log_reg_initial.predict(X_test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qlO8Xb3O9MQs",
        "outputId": "d5879843-23ca-43e4-c606-ce6b9907eb50"
      },
      "outputs": [],
      "source": [
        "print(\"\\nTraining Set Metrics\")\n",
        "compute_print_metrics(y_train, y_train_pred_log)\n",
        "\n",
        "print(\"\\nValidation Set Metrics\")\n",
        "compute_print_metrics(y_val, y_val_pred_log)\n",
        "\n",
        "print(\"\\nTest Set Metrics\")\n",
        "compute_print_metrics(y_test, y_test_pred_log)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Get best hyperparameters using Grid Search."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "83luFnZqDyf8",
        "outputId": "8419ece6-2355-4de0-9980-180540b33350"
      },
      "outputs": [],
      "source": [
        "# Using Grid Serach to find the best parameters\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "\n",
        "log_reg =  LogisticRegression(multi_class='multinomial')\n",
        "\n",
        "lr_param_grid = {\n",
        "    'C': [10, 15, 20],\n",
        "    'solver': ['newton-cg', 'sag', 'saga'],\n",
        "    'max_iter': [90, 100, 150]#, 200, 300, 500, 1000]\n",
        "}\n",
        "\n",
        "\n",
        "grid_search = GridSearchCV(log_reg, param_grid=lr_param_grid, scoring='accuracy', verbose=1, cv=5)\n",
        "\n",
        "\n",
        "# Fit the grid search to the data\n",
        "grid_search.fit(X_train, y_train)\n",
        "grid_search.best_params_"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JirPQMwdFX_A",
        "outputId": "3fa4a515-6be5-477f-ca47-876c9b5a21fe"
      },
      "outputs": [],
      "source": [
        "log_reg = grid_search.best_estimator_\n",
        "\n",
        "# Print the best parameters found\n",
        "print(\"Best parameters found:\", grid_search.best_params_)\n",
        "\n",
        "# Printing the metrics\n",
        "y_train_pred_log_reg = log_reg.predict(X_train)\n",
        "y_val_pred_log_reg = log_reg.predict(X_val)\n",
        "y_test_pred_log_reg = log_reg.predict(X_test)\n",
        "\n",
        "print(\"\\n\\nTraining Set Metrics\")\n",
        "compute_print_metrics(y_train, y_train_pred_log_reg)\n",
        "\n",
        "print(\"\\n\\nValidation Set Metrics\")\n",
        "compute_print_metrics(y_val, y_val_pred_log_reg)\n",
        "\n",
        "print(\"\\nTest Set Metrics\")\n",
        "compute_print_metrics(y_test, y_test_pred_log_reg)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rnQXWpJpLs8u"
      },
      "source": [
        "Performance of the Logistic Regression Classifier:\n",
        "-----------------------------------\n",
        "- **Base Model:**\n",
        "\n",
        "1. Training Set:\n",
        "*   Accuracy:  58.16%\n",
        "*   Precision:  6470%\n",
        "*   Recall:  70.04%\n",
        "*   F1 Score:  58.47%\n",
        "\n",
        "2. Validation Set:\n",
        "*   Accuracy:  57.50%\n",
        "*   Precision:  63.93%\n",
        "*   Recall:  68.54%\n",
        "*   F1 Score:  57.82%\n",
        "\n",
        "3. Test Set:\n",
        "*   Accuracy:  50.00%\n",
        "*   Precision:  55.92%\n",
        "*   Recall:  62.30%\n",
        "*   F1 Score:  50.17%\n",
        "-----------------------------------\n",
        "\n",
        "- **Best Estimator**\n",
        "\n",
        "\n",
        "1. Training Set:\n",
        "*   Accuracy:  76.33%\n",
        "*   Precision:  76.06%\n",
        "*   Recall:  75.21%\n",
        "*   F1 Score: 75.60%\n",
        "\n",
        "1. Validation Set:\n",
        "*   Accuracy:  71.00%\n",
        "*   Precision:  69.88%\n",
        "*   Recall:  69.43%\n",
        "*   F1 Score:  69.51%\n",
        "\n",
        "1. Test Set:\n",
        "*   Accuracy: 62.00%\n",
        "*   Precision: 60.54%\n",
        "*   Recall:  63.02%\n",
        "*   F1 Score: 61.38%\n",
        "-----------------------------------\n",
        "**Findings:**\n",
        "1. Best Hyperparameters: After performing the grid search, the best hyperparameters for logistic regression were found to be C=10,solver='saga' and max_iter = 90.\n",
        "\n",
        "2. The accuracy on the train set of the tuned model was much better (71.1%) than the initial base model (62.3%). The tuned model performed very well compared to the base model on the validation and the test data, with a balanced precision and recall. The f1 score for the tuned model was also much better for all three sets compared to the base model.\n",
        "\n",
        "**Hyperparameters Discussion:**\n",
        "\n",
        "1. Regularization Strength (C):\n",
        "Lower values of C increase the regularization strength, leading to a simpler model with smaller coefficients. This made the model perform worse on the training, testing and validation sets. Higher values of C had the exact opposite effect. The model was overfitting the data and performing slightly better but still too bad on the testing and validation sets.\n",
        "\n",
        "2. Solver: We also experimented with solver, te accurracy, precision, recall and f1 score was did not very much, but based on this particular dataset 'newton-cg' gave the best peroformance.\n",
        "\n",
        "3. max_iter: Trying different values of the max_iter hypermeter we found that the model's performance metrics did not increases significantly after a certain value, but it was taking more time to train. However, the lower values of this hyperparameter resulted in the underftting of the model.\n",
        "\n",
        "4. Based on the number of hyperparameters available and their range of values, there were many available combinations, but Grid Search helped in narraowing down the best values for each."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mSwZDF6dMBqQ"
      },
      "source": [
        "### B. Support vector machines (make sure to try using kernels); hyperparameters to explore: C, kernel, degree of polynomial kernel, gamma"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZDSZQT3uMK5u"
      },
      "source": [
        "SVM Classifiers perform better with scaled features. Features are already scaled using Standard Scaler,"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Train a base classifier first to get a reference to compare the optimized model with best hyperparamters."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2MfrsDErG5nj"
      },
      "outputs": [],
      "source": [
        "from sklearn.svm import SVC\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "# Base model with rbf Kernel\n",
        "poly_kernel_svm_clf = SVC(kernel=\"rbf\", coef0=2, C=25, probability=True, gamma=4)\n",
        "\n",
        "poly_kernel_svm_clf.fit(X_train, y_train)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OHPsHeWQMPo9"
      },
      "outputs": [],
      "source": [
        "print(\"\\n\\nTraining Set Metrics\")\n",
        "compute_print_metrics(y_train, poly_kernel_svm_clf.predict(X_train))\n",
        "\n",
        "print(\"\\n\\nValidation Set Metrics\")\n",
        "compute_print_metrics(y_val, poly_kernel_svm_clf.predict(X_val))\n",
        "\n",
        "print(\"\\n\\nTest Set Metrics\")\n",
        "compute_print_metrics(y_test, poly_kernel_svm_clf.predict(X_test))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "It is obvious that the model is overfitting the training set, but we will use Grid Search to get the best Hyperparameters to avoid overfitting."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dGuaISW5MW3Z"
      },
      "source": [
        "Let's use Grid Search to find the best parameters (including the kernel) for the SVM CLassifier"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bsIJSQ-MMUCs"
      },
      "outputs": [],
      "source": [
        "# Define the model\n",
        "svm_clf = SVC(probability=True)\n",
        "\n",
        "# Define the hyperparameters grid to explore\n",
        "svc_param_grid = {\n",
        "    'kernel': [\"poly\", \"rbf\", \"sigmoid\", ],\n",
        "    'C': [0.005, 0.05, 0.5, 0.1, 1, 1.5, 2, 4, 6],\n",
        "    'gamma': ['scale', 'auto'] + [ 0.001, 0.01, 0.1, 1, 10, 100]\n",
        "}\n",
        "\n",
        "svc_grid_search = GridSearchCV(svm_clf, param_grid=svc_param_grid,cv=10)\n",
        "svc_grid_search.fit(X_train, y_train)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iuXFIoGfMY_j"
      },
      "outputs": [],
      "source": [
        "# Tuned Model\n",
        "svc_clf = svc_grid_search.best_estimator_\n",
        "\n",
        "# Print the best parameters found\n",
        "print(\"Best parameters found:\", svc_grid_search.best_params_)\n",
        "\n",
        "# Printing the metrics for\n",
        "y_train_pred_svc = svc_clf.predict(X_train)\n",
        "y_val_pred_svc = svc_clf.predict(X_val)\n",
        "y_test_pred_svc = svc_clf.predict(X_test)\n",
        "\n",
        "print(\"\\nTraining Set Metrics\")\n",
        "compute_print_metrics(y_train, y_train_pred_svc)\n",
        "\n",
        "print(\"\\nValidation Set Metrics\")\n",
        "compute_print_metrics(y_val, y_val_pred_svc)\n",
        "\n",
        "print(\"\\nTest Set Metrics\")\n",
        "compute_print_metrics(y_test, y_test_pred_svc)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "20InqOf_PxO4"
      },
      "source": [
        "Performance of the SVM Classifier:\n",
        "-----------------------------------\n",
        "- **Base Model:**\n",
        "\n",
        "1. Training Set:\n",
        "*   Accuracy:  100%\n",
        "*   Precision:  100%\n",
        "*   Recall:  100%\n",
        "*   F1 Score:  100%\n",
        "\n",
        "2. Validation Set:\n",
        "*   Accuracy:  54.00%\n",
        "*   Precision:  49.35%\n",
        "*   Recall:  38.30%\n",
        "*   F1 Score:  34.46%\n",
        "\n",
        "1. Test Set:\n",
        "*   Accuracy:  56.00%\n",
        "*   Precision:  62.48%\n",
        "*   Recall:  39.90%\n",
        "*   F1 Score:  37.33%\n",
        "\n",
        "The model clearly overfits the training data. But we will leave it as is as this is not the model we will be using for in the ensemble, it will be our tuned model.\n",
        "\n",
        "-----------------------------------\n",
        "- **Best Estimator**\n",
        "\n",
        "1. Training Set:\n",
        "*   Accuracy:  83.50%\n",
        "*   Precision:  83.18%\n",
        "*   Recall:  82.8%\n",
        "*   F1 Score: 83.02%\n",
        "\n",
        "1. Validation Set:\n",
        "*   Accuracy:  76.%\n",
        "*   Precision:  75.49%\n",
        "*   Recall:  74.68%\n",
        "*   F1 Score:  74.98%\n",
        "\n",
        "1. Test Set:\n",
        "*   Accuracy: 73.50%\n",
        "*   Precision: 72.87%\n",
        "*   Recall:  72.86%\n",
        "*   F1 Score: 72.38%\n",
        "-----------------------------------\n",
        "\n",
        "**Findings:**\n",
        "\n",
        "1. Best Hyperparameters: After performing the grid search, the best hyperparameters for SVM Classifier are C=0.1, gamma=0.1, and kernel = 'ploy'.\n",
        "\n",
        "2. As expected the the accuracy, recall, precision, and f1 score increased for all training, testing, and validation sets after tuning the model.\n",
        "\n",
        "**Hyperparameters Discussion:**\n",
        "\n",
        "1. C: Higher values of C resulted in the overfitting of the model and lower values of C resulted in underfitting.\n",
        "\n",
        "2. gamma: Not all kernels have this parameter. We tried it with 'rbf', 'poly', and 'sigmoid'. The results were similar. Higher value resulted in overfitting and the lower value resulted in underfitting.\n",
        "\n",
        "3. kernel: Different kernels performed very differently. We exerimented with Linear, rbf, poly, and sigmoid kernels. out of which sigmoid kernel gave the best performance. That was also the kernel chosen byy grid search.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F_jeLGrlPz-6"
      },
      "source": [
        "### Random Forest classifier (also analyze feature importance); hyperparameters to explore: the number of trees, max depth, the minimum number of samples required to split an internal node, the minimum number of samples required to be at a leaf node."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Train a base model to compare the optimized best model to."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QtFA7bUgMaoe"
      },
      "outputs": [],
      "source": [
        "# Base Model\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "\n",
        "rf_clf_initial = RandomForestClassifier(n_estimators=10,\n",
        "                                max_depth=3,\n",
        "                                min_samples_split=10,\n",
        "                                min_samples_leaf=15)\n",
        "\n",
        "# Scaling is not required for a Random Forest Classifier\n",
        "rf_clf_initial.fit(X_train, y_train)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HYp1Pq0iP4FE"
      },
      "outputs": [],
      "source": [
        "# Printing the metrics\n",
        "y_train_pred_rf_initial = rf_clf_initial.predict(X_train)\n",
        "y_val_pred_rf_initial = rf_clf_initial.predict(X_val)\n",
        "y_test_pred_rf_initial = rf_clf_initial.predict(X_test)\n",
        "\n",
        "print(\"\\nTraining Set Metrics\")\n",
        "compute_print_metrics(y_train, y_train_pred_rf_initial)\n",
        "\n",
        "print(\"\\nValidation Set Metrics\")\n",
        "compute_print_metrics(y_val, y_val_pred_rf_initial)\n",
        "\n",
        "print(\"\\nTest Set Metrics\")\n",
        "compute_print_metrics(y_test, y_test_pred_rf_initial)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PssuMLtBP7HL"
      },
      "outputs": [],
      "source": [
        "# Feature Importance\n",
        "feature_names = [col for col in df.columns if col != 'Credit_Score']\n",
        "importances = rf_clf_initial.feature_importances_\n",
        "\n",
        "# Create a DataFrame to hold the feature names and their importance scores\n",
        "features_importances_df = pd.DataFrame(zip(feature_names, importances), columns=['Feature', 'Importance'])\n",
        "\n",
        "# Sort the DataFrame by importance in descending order\n",
        "features_importances_df = features_importances_df.sort_values(by='Importance', ascending=False)\n",
        "\n",
        "# Plotting\n",
        "plt.figure(figsize=(10, 8))\n",
        "plt.barh(features_importances_df['Feature'], features_importances_df['Importance'])\n",
        "plt.xlabel('Importance')\n",
        "plt.ylabel('Feature')\n",
        "plt.title('Feature Importances')\n",
        "plt.gca().invert_yaxis()  # Invert y-axis to have the most important feature on top\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Feature Importance from the base model.\n",
        "* Credit Mix is the most important feature of all to predict the Credit Score of a person, followed by Interest Rate, Credit History Age, Number of delayed Payments.\n",
        "* The least important features include Amount invested monthly, Number of Credit Inquiries, Credi Utilization ratio, and Payment of Minimum Amount (Name) (least important)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "P3ULrbOmP80F"
      },
      "outputs": [],
      "source": [
        "# Grid Search to find the best parameters\n",
        "\n",
        "# Define the model\n",
        "rf_clf = RandomForestClassifier(random_state=10)\n",
        "\n",
        "# Define the hyperparameters grid to explore\n",
        "rf_param_grid = {\"n_estimators\": [8, 10, 11],\n",
        "                 \"max_depth\": [5, 10, 12, 18],\n",
        "                 \"min_samples_split\": [8, 9, 10, 12],\n",
        "                 \"min_samples_leaf\": [3, 4, 6]\n",
        "}\n",
        "\n",
        "rf_grid_search = GridSearchCV(rf_clf, param_grid=rf_param_grid,cv=10)\n",
        "rf_grid_search.fit(X_train, y_train)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sRCV9RZXP-7z"
      },
      "outputs": [],
      "source": [
        "rf_clf = rf_grid_search.best_estimator_\n",
        "\n",
        "# Feature Importance\n",
        "feature_names = [col for col in df.columns if col != 'Credit_Score']\n",
        "importances = rf_clf.feature_importances_\n",
        "\n",
        "# Create a DataFrame to hold the feature names and their importance scores\n",
        "features_importances_df = pd.DataFrame(zip(feature_names, importances), columns=['Feature', 'Importance'])\n",
        "\n",
        "# Sort the DataFrame by importance in descending order\n",
        "features_importances_df = features_importances_df.sort_values(by='Importance', ascending=False)\n",
        "\n",
        "# Plotting\n",
        "plt.figure(figsize=(10, 8))\n",
        "plt.barh(features_importances_df['Feature'], features_importances_df['Importance'])\n",
        "plt.xlabel('Importance')\n",
        "plt.ylabel('Feature')\n",
        "plt.title('Feature Importances')\n",
        "plt.gca().invert_yaxis()  # Invert y-axis to have the most important feature on top\n",
        "plt.show()\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Feature Importance from the best estimator model.\n",
        "* Credit Mix is the most important feature of all to predict the Credit Score of a person, followed by Interest Rate, Number of Delayed Payments, and Annual Income.\n",
        "* The least important features include Monthly Balance, Payment of Minimum Amount (Yes), Payment of Minimum Amount (No), and lastly Payment of Minimum Amount (Name)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TBom0HZ6QBZM"
      },
      "outputs": [],
      "source": [
        "# Print the best parameters found\n",
        "print(\"Best parameters found:\", rf_grid_search.best_params_)\n",
        "\n",
        "# Printing the metrics for\n",
        "y_train_pred_rf = rf_clf.predict(X_train)\n",
        "y_val_pred_rf = rf_clf.predict(X_val)\n",
        "y_test_pred_rf = rf_clf.predict(X_test)\n",
        "\n",
        "print(\"\\nTraining Set Metrics\")\n",
        "compute_print_metrics(y_train, y_train_pred_rf)\n",
        "\n",
        "print(\"\\nValidation Set Metrics\")\n",
        "compute_print_metrics(y_val, y_val_pred_rf)\n",
        "\n",
        "print(\"\\nTest Set Metrics\")\n",
        "compute_print_metrics(y_test, y_test_pred_rf)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9PVye9rLQHZl"
      },
      "source": [
        "Performance of the Classifier:\n",
        "-----------------------------------\n",
        "- **Base Model:**\n",
        "\n",
        "1. Training Set:\n",
        "*   Accuracy:  75.67%\n",
        "*   Precision:  74.37%\n",
        "*   Recall:  77.13%\n",
        "*   F1 Score:  75.51%\n",
        "\n",
        "2. Validation Set:\n",
        "*   Accuracy:  65.00%\n",
        "*   Precision:  63.46%\n",
        "*   Recall:  66.59%\n",
        "*   F1 Score:  64.55%\n",
        "\n",
        "3. Test Set:\n",
        "*   Accuracy:  65.00%\n",
        "*   Precision:  63.61%\n",
        "*   Recall:  68.31%\n",
        "*   F1 Score:  64.88%\n",
        "-----------------------------------\n",
        "- **Best Estimator**\n",
        "\n",
        "1. Training Set:\n",
        "*   Accuracy:  87.00%\n",
        "*   Precision:  86.14%\n",
        "*   Recall:  87.22%\n",
        "*   F1 Score:  86.65%\n",
        "\n",
        "1. Validation Set:\n",
        "*   Accuracy:  75.50%\n",
        "*   Precision:  74.57%\n",
        "*   Recall:  75.01%\n",
        "*   F1 Score:  74.68%\n",
        "\n",
        "1. Test Set:\n",
        "*   Accuracy: 73.50%\n",
        "*   Precision: 72.37%\n",
        "*   Recall:  74.68%\n",
        "*   F1 Score: 73.01%\n",
        "-----------------------------------\n",
        "\n",
        "The tuned model performed much better than the base model. The perfromance metrics on the validation and testing set are much higher in the tuned model.\n",
        "\n",
        "**Findings:**\n",
        "1. Best Hyperparameters: After performing the grid search, the best hyperparameters for Random Forest Classifer were max_depth=18, min_samples_leaf=3, min_samples_split=12, n_estimators=11.\n",
        "\n",
        "**Hyperparameters Discussion:**\n",
        "1. max_depth: As expected, the higher values of this hyperparamter was resulting in overfitting and the lower values resulted in underfitting\n",
        "2. min_samples_leaf: Higher values made the model perform better as the classifier was able to generalie better. It prevented model to create leaves that are too specific and the model was able to perfrorm better on the validation and training sets.\n",
        "3. min_samples_split: Higher value of this hyperparameter put more restriction on splitting the nodes resulting in less dense trees. The model was able to generalize better and performed well on the testing and validation sets.\n",
        "4. n_estimators: It specifies the number of trees in the ensemble. We kept increasing its value and saw that, the more trees the better, but after a certain point increasing the value was not changing the model's performance metrics.\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yRzcHuOHQL6m"
      },
      "source": [
        "## 5. Combine your classifiers into an ensemble and try to outperform each individual classifier on the validation set. Once you have found a good one, try it on the test set. Describe and discuss your findings."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-MWM6z0NQOKq"
      },
      "source": [
        "We will combine the Logistic Regression, Support Vector Machine Classifer, and the Random Forest Classifier into an ensemble of Soft Voting Classfier. Soft Voting Classifier are generally better than a Hard Voting Classifier"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GmXkrzV7QFaz"
      },
      "outputs": [],
      "source": [
        "# function to compare the validation set metrics for an ensemble\n",
        "\n",
        "def compare_ensemble_val(ensemble, X_val, y_val):\n",
        "    print(\"--------------------------------------------------------------------\")\n",
        "    print(\"Validation Set Metrics\")\n",
        "    print(\"--------------------------------------------------------------------\")\n",
        "    y_val_pred_ensemble = ensemble.predict(X_val)\n",
        "    print(f\"\\nEnsemble: {ensemble}\")\n",
        "    compute_print_metrics(y_val, y_val_pred_ensemble)\n",
        "\n",
        "\n",
        "    for clf in (log_reg, svc_clf, rf_clf):\n",
        "        y_pred_est = clf.predict(X_val)\n",
        "        print(f\"\\nEstimator: {clf}\")\n",
        "        compute_print_metrics(y_val, y_pred_est)\n",
        "\n",
        "def compare_ensemble_val_and_test(ensemble, X_val, y_val, X_test, y_test):\n",
        "    print(\"--------------------------------------------------------------------\")\n",
        "    print(\"Validation Set Metrics\")\n",
        "    print(\"--------------------------------------------------------------------\")\n",
        "    y_val_pred_ensemble = ensemble.predict(X_val)\n",
        "    print(f\"\\nEnsemble: {ensemble}\")\n",
        "    compute_print_metrics(y_val, y_val_pred_ensemble)\n",
        "\n",
        "\n",
        "    for clf in (log_reg, svc_clf, rf_clf):\n",
        "        y_pred_est = clf.predict(X_val)\n",
        "        print(f\"\\nEstimator: {clf}\")\n",
        "        compute_print_metrics(y_val, y_pred_est)\n",
        "    print(\"--------------------------------------------------------------------\")\n",
        "    print(\"Test Set Metrics\")\n",
        "    print(\"--------------------------------------------------------------------\")\n",
        "    y_test_pred_ensemble = ensemble.predict(X_test)\n",
        "    print(f\"\\nEnsemble: {ensemble}\")\n",
        "    compute_print_metrics(y_test, y_test_pred_ensemble)\n",
        "\n",
        "\n",
        "    for clf in (log_reg, svc_clf, rf_clf):\n",
        "        y_pred_est = clf.predict(X_test)\n",
        "        print(f\"\\nEstimator: {clf}\")\n",
        "        compute_print_metrics(y_test, y_pred_est)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Soft-Voting Classifier"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cejTzd1YQQQj"
      },
      "outputs": [],
      "source": [
        "from sklearn.ensemble import VotingClassifier\n",
        "\n",
        "soft_voting_clf = VotingClassifier(\n",
        "    estimators = [(\"lr\", log_reg), (\"svc\", svc_clf), (\"rf\", rf_clf)],\n",
        "    voting=\"soft\"\n",
        ")\n",
        "\n",
        "soft_voting_clf.fit(X_train, y_train)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uz9-3oFkQR99"
      },
      "outputs": [],
      "source": [
        "print(\"Soft Voting Classifier Performance Metrics\")\n",
        "print(\"Training Set Metrics\")\n",
        "compute_print_metrics(y_train, soft_voting_clf.predict(X_train))\n",
        "\n",
        "print(\"\\n\\nValidation Set Metrics\")\n",
        "compute_print_metrics(y_val, soft_voting_clf.predict(X_val))\n",
        "\n",
        "#print(\"\\n\\nTest Set Metrics\")\n",
        "#compute_print_metrics(y_test, soft_voting_clf.predict(X_test))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tMU7Y3mRQUYA"
      },
      "outputs": [],
      "source": [
        "compare_ensemble_val(soft_voting_clf, X_val, y_val)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Soft-voting classifier did not outperform the individual classifiers.\n",
        "Here is the summary of it's perfomance on the training set and validation set:\n",
        "\n",
        "-- Training Set Metrics\n",
        "* Accuracy:  83.67%\n",
        "* Precision:  83.19%\n",
        "* Recall:  83.31%\n",
        "* F1 Score:  83.25%\n",
        "\n",
        "\n",
        "-- Validation Set Metrics\n",
        "* Accuracy:  74.00\n",
        "* Precision:  73.41%\n",
        "* Recall:  72.63%\n",
        "* F1 Score:  72.91%"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F1eEghl6bpCT"
      },
      "source": [
        "### Hard Voting Classifier"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "puRVuQ4DQXyu"
      },
      "outputs": [],
      "source": [
        "from sklearn.ensemble import VotingClassifier\n",
        "\n",
        "hard_voting_clf = VotingClassifier(\n",
        "    estimators = [(\"lr\", log_reg), (\"svc\", svc_clf), (\"rf\", rf_clf)],\n",
        "    voting=\"hard\"\n",
        ")\n",
        "\n",
        "hard_voting_clf.fit(X_train, y_train)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nY0XcoCQbs9E"
      },
      "outputs": [],
      "source": [
        "print(\"Hard Voting Classifier Performance Metrics\")\n",
        "print(\"Training Set Metrics\")\n",
        "compute_print_metrics(y_train, hard_voting_clf.predict(X_train))\n",
        "\n",
        "print(\"\\n\\nValidation Set Metrics\")\n",
        "compute_print_metrics(y_val, hard_voting_clf.predict(X_val))\n",
        "\n",
        "#print(\"\\n\\nTest Set Metrics\")\n",
        "#compute_print_metrics(y_test, hard_voting_clf.predict(X_test))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1IdoX9FwbuXp"
      },
      "outputs": [],
      "source": [
        "compare_ensemble_val(hard_voting_clf, X_val, y_val)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Hard-Voting Classifier also did not outperform the individual classifiers.\n",
        "Here is the summary of its performance on the training and validation sets:\n",
        "\n",
        "-- Training Set Metrics\n",
        "* Accuracy:  83.67%\n",
        "* Precision:  83.31%\n",
        "* Recall:  83.09%\n",
        "* F1 Score:  83.20%\n",
        "\n",
        "\n",
        "-- Validation Set Metrics\n",
        "* Accuracy:  74.50%\n",
        "* Precision:  74.04%\n",
        "* Recall:  72.95%\n",
        "* F1 Score:  73.34%"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TW9rzbtSb7SC"
      },
      "source": [
        "### Stacking"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FfHSzohKby58"
      },
      "outputs": [],
      "source": [
        "from sklearn.ensemble import StackingClassifier\n",
        "\n",
        "stack = StackingClassifier(estimators = [(\"lr\", log_reg), (\"svc\", svc_clf), (\"rf\", rf_clf)],\n",
        "                           final_estimator=LogisticRegression(C=0.01, multi_class='multinomial', solver='newton-cg'))\n",
        "\n",
        "stack.fit(X_train, y_train)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DkAhaZSCb8yz"
      },
      "outputs": [],
      "source": [
        "print(\"Stacking Classifier Performance Metrics\")\n",
        "print(\"Training Set Metrics\")\n",
        "compute_print_metrics(y_train, stack.predict(X_train))\n",
        "\n",
        "print(\"\\n\\nValidation Set Metrics\")\n",
        "compute_print_metrics(y_val, stack.predict(X_val))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-4L_i4IKb-tC"
      },
      "outputs": [],
      "source": [
        "compare_ensemble_val(stack, X_val, y_val)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Stacking, like other ensembles we tred did not out-perform the individual classifiers.\n",
        "Here is the summary of the stacking ensemble:\n",
        "\n",
        "-- Training Set Metrics\n",
        "* Accuracy:  80.50%\n",
        "* Precision:  87.71%\n",
        "* Recall:  74.09%\n",
        "* F1 Score:  78.30%\n",
        "\n",
        "\n",
        "-- Validation Set Metrics\n",
        "* Accuracy:  70.50%\n",
        "* Precision:  76.11%\n",
        "* Recall:  63.05%\n",
        "* F1 Score:  66.47%"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Comparing test set performance of best ensemble with the inividual classfiers"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Since, Soft voting classifier performed better with a better precision and recall, we will use Soft-Coting Classifier to comapre the performance of an ensemble versus the individual classifiers."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "compare_ensemble_val_and_test(soft_voting_clf, X_val, y_val, X_test, y_test)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "On the test set, the soft-voting classifier did perform better than the Logistic Regression, but it could not out-perform the Support Vector Machine Classifier and Random Forest Classifier."
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
